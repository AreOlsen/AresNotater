# Quicksort.
[[Big theta]]

Likt som [[Mergesort]], bruker ogs√• quicksort [[Divide and Conquer.]] strukturen s√• den er ogs√• ein rekursiv algoritme. Men m√•ten quicksort bruker divide and conquer er litt forskjellig i fra mergesort. I mergesort bruker divide steget s√• vidt noko i det heile, og alt det vanskelige skjer i combine steget. Quicksort derimot er det omvendte, all det vanskelige arbeidet skjer i divide steget. Honestly combine stegt i quicksort gjer avsolutt ingenting, no like seriously.

Quicksort har norke n√∏kkeldifferanser i fr√• mergeSort. Quicksort jobber in place. Og dens worst-case running time er like gale som [[Selection sort.]] og [[Insertion sort.]] $\theta(n^2)$. Men dens gjennomsnittlege-case running time er like god som mergeSort sin $\theta(n\times \log_2(n))$. 

S√• kvifor tenker vi p√• quicksort n√•r vi har eit like godt alternativ mergeSort? 
Dette er fordi den konstante faktoren som er gj√∏mt i [[Big theta]] notasjonen for quicksort er ganske god.. I praktisk pleier quicksort √• gj√∏re det betre enn mergeSort og den gjer det mykje betre enn selection sort og insertion sort.

Her kan vi sj√• korleis quicksort bruker divide and conquer strukturen. Likt som med mergeSort tenk p√• ein subarray $array[p..r]$, der den originale subarrayen er $array[0..n-1]$. 
1. Divide ved √• velge eit tilfeldig element i subarrayen $array[p..r]$. Kall dette elementet v√•r pivot point.
	1. Rearranger alle elementene i $array[p..r]$ slik at alle elementene i $array[p..r]$ er mindre eller lik pivotpointen til venstre for den, og alle som er h√∏gere til h√∏gre for den. Dette blir kalt for **partitioning**. P√• dette punktet er det ikkje viktig kva rekkef√∏lge elementene til venstre av pivotpointet har til felles med kvarandre, eller likt til h√∏gre for pivot pointet. Vi bryr oss berre om kvart element er p√• rett side av pivotpointet.
	2. Vi oftast pleier √• velge v√•rt pivotpoint som det siste elementet i subarrayen.
2. Conquer, ved √• rekursivt sortere subarrayen sin $array[p..q-1]$ (Alle elementer til venstre for pivotpointet, som m√• vere mindre eller lik pivotpointet) og $array[q+1..r]$ (Alle elementer til h√∏gre for pivot pointet, som m√• vere st√∏rre eller lik pivotpointet).
3. Kombiner ved √• gjere ingenting, fordi vi har allereie sortert den i gjennom divide og conquer steget. Combine steget i quicksort combine ingenting!

Base-casen er n√•r subararrayen er av st√∏rrelse mindre enn to elementer, akkurat som i mergeSort. I merge, ser du aldri ein subarray med 0 elementer, men det kan du gjere i quicksort dersom alle andre elementer er entens til venstre eller h√∏gre for pivotpointet n√•r det sortert.

Slik utvikler quicksort seg:
![](https://cdn.kastatic.org/ka-perseus-images/9876d4dc59e01a4742860ae1831c20f654ed7959.png)

Vi velger eit pivotpoint, vi rearrangerer, vi velger eit nytt pivot point, vi rearrangerer, repiter til vi har ein subarray av element st√∏rrelse lavere enn 2. Sorter denne, det er jo berre √• sjekke kass av dei to som er st√∏rst og plassere riktig. 

## Linear-time partiotioning.
Det virkelige arbeidet av quicksort skjer i l√∏pet av divider steget, som partione $array[p..r]$ rundt eit pivot point dratt i fr√• subarrayen. Likevel vi kan velge kva enn som helst element i subarrayen som pivot pointet, er det enklest √• partione viss vi velger det h√∏gste index elementet av subarray $array[p..r]$. 

Etter at vi har v√•rt pivot point kan vi partione subarrayen ved √• g√• gjennom den fr√• venstre til h√∏gre og samenligne kvart element med pivoten, har har to indices $q$ og $j$ in i subarrayen som dividerer den opp i $4$ grupper. Vi kan bruke variabeln $q$ fordi indexen vil eventuellt vere p√• v√•rt pivot point. Vi kan bruke $j$ fordi det er ein teller variabel og variabelen blir kastet vekk n√•r vi er ferdig.

* Alle elementer i $array[p..q-1]$ er gruppe "L", som best√•r av elementer kjent til √• vere mindre eller lik til pivoten.
* Alle elementer i subarrayen $array[q..j-1]$ er gruppe "G" som best√•r av elementer kjent til √• vere st√∏rre enn pivoten.
* Elementer i subarray $array[j..r-1]$ er kjent som gruppe "U", og best√•r av elementer som sitt forhold til pivtoen er ukjent, fordi dei har ikkje enda blitt compared.
* $array[r]$ er gruppe "P", pivot pointet.

Originalt er b√•de $q$ og $j$ lik som $p$. Ved kvart steg, samanliknar vi $array[j]$, den mest venstre elementet i gruppe "U" med pivoten. Viss $array[j]$ er st√∏rre enn pivoten s√• $j++$, slik at linjen som dividerer gruppe "G" og gruppe "U" g√•r ein posisjon over til h√∏gre.
![](https://cdn.kastatic.org/ka-perseus-images/557d3afb93f7ba9abb3a9e845113fcf9ce1e56d8.png)

N√•r vi kommer til pivoten s√• er gruppe "U" tom. Vi deretter swapper pivoten med den mest venstre elementet i gruppe "G": swap $array[r]$ med $array[q]$. Dette plasserer pivoten mellom gruppene "L" og "G" og gjer den rette tingen til og med viss gruppe "L" eller gruppe "G" er tom.

Denne "partition" funksjonen som implementerer denne ideen returner og index $q$ der den endte opp med √• plassere pivoten, slik at $quicksort$ funksjonen, som blir kalt, veit kor partitionsene er.

Her er dei f√∏rste stegene i partione subarray $[12,7,14,9,10,11]$ i $array[4..9]$

![](https://cdn.kastatic.org/ka-perseus-images/53692155715c9f26ec927cb2d40e70ce6c460e86.png)

√Ö partionene ein subarray med $n$ element vil ta $\theta(n)$ tid. Kvart element $array[j]$ er compared ein gang med pivoten. $array[j]$ kanskje eller kanskje ikke blir swapper med $array[q]$, og $q$ kanskje eller kanskje ikkje blir inkrementert, og $j$ blir alltid inkrementert. Den totale nummeret av linjer executer per element av subarrayen er konstant. Siden subarrayen har $n$ vil den totale partition tiden vere $\theta(n)$: linear-time partioning.


# Analyse av quicksort.
Satan fuck deg quicksort. ü§Ø.

Quicksort sin verse case sceneario og gjennomsnittleg scenario er forskjellig p√• grunn av metoden vi finner v√•rt pivot-point som lar oss redusere tidsbruken ganske mykje.
Viss vi er ganske uheldig og partition st√∏rrelse er j√¶vlig ubalansert, i vertste tilfelle der den eine er $0$ og den andre $n-1$ , alle utanom pivotpointet, s√• vil det vere [[Rekurisve algoritmer.|rekurisve tilkallinger p√• st√∏rrelser av n-1 og 0.]]. Likt som i [[Mergesort]] er denne tidne det samme som $\theta(n)$ p√• ein array av st√∏rrelse $n$. I mergesort er det tiden for meging, men i quicksort er det **berre** tiden for partitioning.

## Verste-case sceneario running time.
N√•r quicksort alltid har den mest ubalanserte partitionene mogleg s√• er den originale tilkallingen $cn$ tid for ein konstant av $c$, og der er $n-1$ rekurisve tilkallinger, den f√∏rste vil dermed vere $c(n-1)$ den neste $c(n-2)$ ‚Ä¶‚Ä¶ til det er $c(1)$. Slik vil denne subproblem grafen sj√• ut.
![](https://cdn.kastatic.org/ka-perseus-images/7da2ac32779bef669a6f05decb62f219a9132158.png)

N√•r vi tar og adderer alle dei ulike running-timene vil det sj√• slik ut:
$cn+c(n-1)+c(n-2)‚Ä¶‚Ä¶+2c=c(n+(n-1)+(n-2)+(n-3)‚Ä¶‚Ä¶)=c((n+1)(\frac{n}{2})-1)$.
Den siste linjen $1+2+3‚Ä¶‚Ä¶‚Ä¶‚Ä¶+n$ er ein aritmetikk serier, og som vi s√•g n√•r vi analyserte [[Selection sort.]]  (Vi subtraherer 1 fordi for quicksort, starter summasjonen p√• 2, og ikkje 1). Vi har nokre lavere order termer og konstante koeffisienter, men n√•r vi bruker [[Big theta]] notasjon s√• ignorer vi dem fordi dei er konstante verdier og lavere termer. I [[Big theta]] notasjon er quicksort sin verste kj√∏re tid $\theta(n^2)$.

## Beste-case sceneario running time.

Quicksort sin beste case skjer n√•r partionene er s√• gjevnt som mogleg, der deirast sider er entens like eller med ein distanse av 1 i st√∏rrelse av kvarandre. Casen med ingen forskjell i forskjell skjer n√•r subarrayen har eit oddetall nummer av elementer og pivoten er rett i midten etter partionering, og kvart partition har $\huge \frac{n-1}{2}$ elementer. Den andre casen, skjer viss subarrayen har eit partall nummer av elementer og pivoten den eine siden/partionen har $\huge \frac{n}{2}$ elementer, mens den andre har $\huge \frac{n}{2}-1$ elementer. I kvar av disse casene har partionen p√• det meste $\huge \frac{n}{2}$ elementer, og dens tre/graf av subproblemene sine st√∏rrelse ser meir ut som treet for subproblemer i [[Mergesort]], med dens partionerings tider liknande til merging tidene:
![](https://cdn.kastatic.org/ka-perseus-images/21cd0d70813845d67fbb11496458214f90ad7cb8.png)

Vi bruker [[Big theta]] notasjon og f√•r det samme resultatet som for mergesort $\theta(n\times \log_2(n))$.

## Gjennomsnittleg-case scenario running-time.
For √• vise at den gjennomsnittlege running timen er √≤g $\theta(n\times \log_2(n))$ krever det faktisk nokre ganske avansert matematikk s√• vi g√•r ikkje der. Men vi kan forst√• litt betre korleis denne algoritmen sin generelle kj√∏re tid er dersom vi ser p√• nokre ulike d√∏mer.

F√∏rst la oss forestille oss at vi ikkje alltid f√•r heilt perfekte balanserte partitionser, vi f√•r til d√∏mes p√• det verste ein split av $3$ til $1$. Det er betyr den eine siden f√•r $\frac{3}{4}\times n$ elementer, og den andre $\frac{1}{4} \times n$ elementer. (Vi ignorer pivot pointet her for √• holde matte ren, men den gjer at vi ikkje f√•r egentelig ein perfekt 3 til 1 split). Da vil treet av subproblemer og partition tidene se slik ut:
![](https://cdn.kastatic.org/ka-perseus-images/130b2d2a1fe897253def054f4c3aa7bd94cb6cf2.png)


Venstre childen av ein node representerer eit subproblem av st√∏rrelse $\frac{1}{4}$ av forrige, og childen til h√∏gre viser $\frac{3}{4}$ av forrige. Viss vi f√∏lger venstre sidene vil vi raskt minske ned til basecasen av $1$ fordi vi har mykje mindre elementer per iterasjon, mens derimot viss vi f√∏lger berre til h√∏gre vil vi mykje saktere komme til basecasen av $1$. Som figuren viser etter $\log_4(n)$ leveler kjemer vi ned til st√∏rrelsen av $1$ viss vi f√∏lger til venstre. Det er enklest √• forestille seg kvifor $\log_4(n)$  viss vi starter med $1$ og g√•r opp med $4\times$ om gangen  til vi n√•r $n$. S√• i andre ord sp√∏r vi kva verdi av $x$ vi f√•r $4^x=n$, og svaret er jo da sj√∏lvsagt $\log_4(n)$. Derimot n√•r vi g√•r nedover til h√∏gre kvar iterasjon vil svaret vere $\log_\frac{4}{3}(n)$ til vi kommer ned til basecasen av $1$. For √• f√∏restille seg kvifor akkurat $\log_\frac{4}{3}(n)$ er det lett √• tenke at kvart barn er $\frac{3}{4}$ av den √∏vre noden sin st√∏rrelse, kvar √∏vre node er  dermed $\frac{4}{3}$ av barnets st√∏rrelse. Viss vi dermed starter med ein basecase av $1$ og multiplserer med $\frac{4}{3}$ til vi n√•r $n$ s√• har vi multiplisert $\log_\frac{4}{3}(n)$ ganger. 

Den verste runningtimen av $\log_4(n)$ og $\log_\frac{4}{3}(n)$ er $\log_\frac{4}{3}(n)$ s√• vi fokuserer berre p√• er $\log_\frac{4}{3}(n)$.For kvart av dei f√∏rste $\log_4(n)$ niv√•ene er det $n$ elementer (inkludert pivotpointer som i realiteten ikkje blir partionet.) og s√• den totale partition tiden er for kvar av disse niv√•ene berre $cn$ for ein konstant av $c$.  Og dei siste niv√•ene har mindre enn $n$ noder og s√• for kvar av disse levelene er det p√• det meste $cn$ runningtime. Til sammen er det $\log_\frac{4}{3}(n)$ eveler, og s√• den totale partition tiden er $O(n \times \log_\frac{4}{3}(n))$. 

Og det er eit matematisk bevis som nemner at $\log_a(n)=\frac{\log_b(n)}{\log_b(a)}$., som tyder at vi berre har ein konstant faktor mellom dei ulike tallene og vi kan egentelig berre bestemme kva enn som helst base vi √∏nsker √• ha n√•r vi noterer ned runningtimen. S√• vi pleier √• bruke $2$ for basen. Dermed er quicksort sin running time $O(\log_2(n))$, likevel vi faktisk har ein h√∏gere konstant faktor som er h√∏gere enn den beste case running timen.

Kor ofte vi burde forvente √• see ein split som er $3$ til $1$ eller betre? Det kjemer heilt ann p√• korleis vi velger v√•re pivot-points. Viss vi forestiller oss at pivotpintenhar ein like stor sjanse for √• lande p√• kva enn som helst index av ein $n$ elementer liste. Da m√• vi pivotpointen lande i midten av arrayen for √• oppn√• ein split som er $3$ til $1$ eller betre.
![](https://cdn.kastatic.org/ka-perseus-images/a77e296baf334b577328a5f15ab294a652ad6b96.png) 

S√• viss pivot-pointen har ein like stor sjanse for √• lande kor enn som helst i arrayen etter partitioning er det ein $50\%$ sjanse for at vi f√•r p√• det verste ein split av $3$ til $1$.

Viss vi tar eit anna d√∏me og ser p√• d√∏met der vi f√•r ein alternerende split av $3$ til $1$ og den verste case scenario, og vi tenker p√• ein node i eit tre med $k$ elementer i dens subarray. Da vil treet sj√• slik ut:
![](https://cdn.kastatic.org/ka-perseus-images/1f6a230039af38419453096c58bfd3ab5ac77b0f.png)

I staden for √• sj√• slik ut:
![](https://cdn.kastatic.org/ka-perseus-images/656ca0d8ab950299dcc2800e6b2a52cc7ea6ffd5.png)


Dermed viss vi f√•r den verste case splitten halvparten av tiden og ein split som $3$ til $1$ halvparten av tiden vil runningtimen vere dobbel s√• stor av det √• f√• ein runningtime av $3$ til $1$ kvar gang. Igjen s√• er dette berre ein konstant faktor som blir absorbert inn i [[Big O]] notasjonen, dermed er √≤g running timen i denne scenarioet $O(n\times \log_2(n))$.

